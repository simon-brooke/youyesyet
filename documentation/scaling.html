<!DOCTYPE html PUBLIC ""
    "">
<html><head><meta charset="UTF-8" /><title>YouYesYet: Scaling</title><link rel="stylesheet" type="text/css" href="css/default.css" /><link rel="stylesheet" type="text/css" href="css/highlight.css" /><script type="text/javascript" src="js/highlight.min.js"></script><script type="text/javascript" src="js/jquery.min.js"></script><script type="text/javascript" src="js/page_effects.js"></script><script>hljs.initHighlightingOnLoad();</script></head><body><div id="header"><h2>Generated by <a href="https://github.com/weavejester/codox">Codox</a></h2><h1><a href="index.html"><span class="project-title"><span class="project-name">Youyesyet</span> <span class="project-version">0.2.1</span></span></a></h1></div><div class="sidebar primary"><h3 class="no-link"><span class="inner">Project</span></h3><ul class="index-link"><li class="depth-1 "><a href="index.html"><div class="inner">Index</div></a></li></ul><h3 class="no-link"><span class="inner">Topics</span></h3><ul><li class="depth-1 "><a href="authorisation.html"><div class="inner"><span>Security and authorisation</span></div></a></li><li class="depth-1 "><a href="competitors.html"><div class="inner"><span>Competitor Analysis</span></div></a></li><li class="depth-1 "><a href="database.html"><div class="inner"><span>Database Specification</span></div></a></li><li class="depth-1  current"><a href="scaling.html"><div class="inner"><span>YouYesYet: Scaling</span></div></a></li><li class="depth-1 "><a href="userspec.html"><div class="inner"><span>YouYesYet: User-oriented specification</span></div></a></li></ul><h3 class="no-link"><span class="inner">Namespaces</span></h3><ul><li class="depth-1"><div class="no-link"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>youyesyet</span></div></div></li><li class="depth-2 branch"><a href="youyesyet.config.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>config</span></div></a></li><li class="depth-2"><div class="no-link"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>db</span></div></div></li><li class="depth-3"><a href="youyesyet.db.core.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>core</span></div></a></li><li class="depth-2 branch"><a href="youyesyet.handler.html"><div class="inner"><span class="tree" style="top: -52px;"><span class="top" style="height: 61px;"></span><span class="bottom"></span></span><span>handler</span></div></a></li><li class="depth-2 branch"><a href="youyesyet.layout.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>layout</span></div></a></li><li class="depth-2 branch"><a href="youyesyet.locality.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>locality</span></div></a></li><li class="depth-2 branch"><a href="youyesyet.middleware.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>middleware</span></div></a></li><li class="depth-2 branch"><a href="youyesyet.oauth.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>oauth</span></div></a></li><li class="depth-2 branch"><a href="youyesyet.outqueue.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>outqueue</span></div></a></li><li class="depth-2"><div class="no-link"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>routes</span></div></div></li><li class="depth-3 branch"><a href="youyesyet.routes.auto.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>auto</span></div></a></li><li class="depth-3 branch"><a href="youyesyet.routes.auto-json.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>auto-json</span></div></a></li><li class="depth-3 branch"><a href="youyesyet.routes.home.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>home</span></div></a></li><li class="depth-3 branch"><a href="youyesyet.routes.issue-experts.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>issue-experts</span></div></a></li><li class="depth-3 branch"><a href="youyesyet.routes.logged-in.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>logged-in</span></div></a></li><li class="depth-3 branch"><a href="youyesyet.routes.manual.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>manual</span></div></a></li><li class="depth-3 branch"><a href="youyesyet.routes.oauth.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>oauth</span></div></a></li><li class="depth-3 branch"><a href="youyesyet.routes.rest.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>rest</span></div></a></li><li class="depth-3 branch"><a href="youyesyet.routes.roles.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>roles</span></div></a></li><li class="depth-3"><a href="youyesyet.routes.services.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>services</span></div></a></li><li class="depth-2 branch"><a href="youyesyet.utils.html"><div class="inner"><span class="tree" style="top: -331px;"><span class="top" style="height: 340px;"></span><span class="bottom"></span></span><span>utils</span></div></a></li><li class="depth-2"><a href="youyesyet.validation.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>validation</span></div></a></li></ul></div><div class="document" id="content"><div class="doc"><div class="markdown"><h1><a href="#youyesyet-scaling" name="youyesyet-scaling"></a>YouYesYet: Scaling</h1>
<p>Suppose the YouYesYet project works and we have thousands or tens of thousands of volunteers across Scotland all out chapping doors at the same time: how do we ensure the system stays up under load?</p>
<h2><a href="#sizing-the-problem" name="sizing-the-problem"></a>Sizing the problem</h2>
<p>There’s no point in building the app if it will break down under load. We need to be persuaded that it is possible to support the maximum predictable load the system might experience.</p>
<h3><a href="#database-load-per-volunteer" name="database-load-per-volunteer"></a>Database load per volunteer</h3>
<p>A street canvasser visits on average while working not more than one dwelling every two minutes; the average doorknock-to-doorknock time is probably more like five minutes. Each visit results in</p>
<ol>
  <li>Zero or one visit record being created;</li>
  <li>Zero to about five intention records;</li>
  <li>Zero to about five followup request records.</li>
</ol>
<p>So in aggregate minimum zero, maximum about eleven records, typical probably one visit, two intentions = three database inserts per street volunteer per visit. Telephone canvassers probably achieve slightly more because they don’t have to walk from door to door. But over all we’re looking at an average of less than one insert per volunteer per minute.</p>
<p>Database reads are probably more infrequent. Each client will obviously need to download the data for each dwelling visited, but it will download these in geograhic blocks of probably around 100 dwellings, and will download a new block only when the user goes outside the area of previously downloaded blocks. However, there ideally should be frequent updates so that the canvasser can see which dwellings other members of the team have already visited, in order that the same dwelling is not visited repeatedly. So there’s probably on average one database read per visit.</p>
<h3><a href="#reliability-of-network-links" name="reliability-of-network-links"></a>Reliability of network links</h3>
<p>Mobile phones typically can have intermittent network access. The client must be able to buffer a queue of records to be stored, and must not prevent the user from moving on to the next doorstep just because the data from the last visit has not yet been stored. There should probably be some on-screen indication of when there is unsent buffered data.</p>
<h3><a href="#pattern-of-canvassing" name="pattern-of-canvassing"></a>Pattern of canvassing</h3>
<p>Canvassing takes place typically between 6:30pm and 9:00pm on a weekday evening. There will be some canvassing outside this period, but not enough to create significant load. Canvassing will be higher on dry nights than on wet ones, and will probably ramp up through the campaign.</p>
<h3><a href="#total-number-of-volunteers" name="total-number-of-volunteers"></a>Total number of volunteers</h3>
<p>Personally I’ve never worked in a big canvassing team - maximum about forty people. I believe that there were bigger teams in some parts of urban Scotland. I would guess that the maximum number of volunteers canvassing at any one time - across all groups campaigning for ‘Yes’ in the first independence referendum - never exceeded 35,000 and was probably much lower. I’ve asked whether anyone has better figures but until I have a better estimate I’m going to work on the basis of 35,000 maximum concurrent users.</p>
<h3><a href="#estimated-peak-transactions-per-second" name="estimated-peak-transactions-per-second"></a>Estimated peak transactions per second</h3>
<p>This means that the maximum number of transactions per second across Scotland is about</p>
<pre><code>35,000 * (1 + 0.2)
------------------ = 700 transactions per second
        60
</code></pre>
<p>700 transactions per second is not a very large number. We should be able to support this level of load on a single server. But what if we can’t?</p>
<h2><a href="#spreading-the-load" name="spreading-the-load"></a>Spreading the load</h2>
<h3><a href="#caching-and-memoizing" name="caching-and-memoizing"></a>Caching and memoizing</h3>
<p>People typically go out canvassing in teams; each member of the team will need the same elector data.</p>
<p>Glasgow has a population density of about 3,260 per Km^2; that means each half kilometer square has a maximum population of not much more than 1,000. Downloading 1,000 elector records at startup time is not infeasible. If we normalise data requests to a 100 metre square grid and serve records in 500 metre square chunks, all the members of the same team will request the same chunk of data. Also, elector data is not volatile. Therefore it makes sense to memoize requests for elector data. The app should only request fresh elector data when the device moves within 100 metres of the edge of the current 500 metre cell.</p>
<p>Intention data is volatile: we’ll want to update canvassers with fresh intention data frequently, because the other members of their team will be recording intention data as they work, and it’s by seeing that intention data that the canvassers know which doors are still unchapped. So we don’t want to cache intention data for very long. But nevertheless it still makes sense to deliver it in normalised 500 metre square chunks, because that means we can temporarily cache it server side and do not actually have to hit the database with many requests for the same data.</p>
<p>Finally, issue data is not volatile over the course of a canvassing session, although it may change over a period of days. So issue data - all the current issues - should be fetched once at app startup time, and not periodically refreshed during a canvassing session. Also, of course, every canvasser will require exactly the same issue data (unless we start thinking of local or regional issues…?), so it absolutely makes sense to memoise requests for issue data.</p>
<p>All this normalisation and memoisation reduces the number of read requests on the database.</p>
<p>Note that <a href="https://github.com/clojure/core.memoize">clojure.core.memoize</a> provides us with functions to create both size-limited, least-recently-used caches and duration limited, time-to-live caches.</p>
<h3><a href="#searching-the-database-for-localities" name="searching-the-database-for-localities"></a>Searching the database for localities</h3>
<p>At 56 degrees north there are 111,341 metres per degree of latitude, 62,392 metres per degree of longitude. So a 100 metre box is about 0.0016 degrees east-west and .0009 degrees north-south. If we simplify that slightly (and we don’t need square boxes, we need units of area covering a group of people working together) then we can take .001 of a degree in either direction which is computationally cheap.</p>
<p>Of course we could have a search query like this</p>
<pre><code>select * from addresses
  where latitude &gt; 56.003
    and latitude &lt; 56.004
    and longitude &gt; -4.771
    and longitude &lt; -4.770;
</code></pre>
<p>And it would work - but it would be computationally expensive. If we call each of these .001 x .001 roughly-rectangles a <strong>locality</strong>, then we can give every locality an integer index as follows</p>
<pre><code>(defn locality-index
  "Compute a locality for this `latitude`, `longitude` pair."
  [latitude longitude]
  (+
    (* 10000            ;; left-shift the latitude component four digits
      (integer
        (* latitude 1000)))
    (-                  ;; invert the sign of the longitude component, since
                        ;; we're interested in localities West of Greenwich.
      (integer
        (* longitude 1000)))))
</code></pre>
<p>For values in Scotland, this gives us a number comfortable smaller than the maximum size of a 32 bit integer. Note that this isn’t generally the case, so to adapt this software for use in Canada, for example, a more general solution would need to be chosen; but this will do for now. If we compute this index at the time the address is geocoded, then we can achieve the exact same results as the query given above with a much simpler query:</p>
<pre><code>select * from address where locality = 560034770;
</code></pre>
<p>If the locality field is indexed (which obviously it should be) this query becomes very cheap.</p>
<h3><a href="#geographic-sharding" name="geographic-sharding"></a>Geographic sharding</h3>
<p>Volunteers canvassing simultaneously in the same street or the same locality need to see in near real time which dwellings have been canvassed by other volunteers, otherwise we’ll get the same households canvassed repeatedly, which wastes volunteer time and annoys voters. So they all need to be sending updates to, and receiving updates from, the same server. But volunteers canvassing in Aberdeen don’t need to see in near real time what is happening in Edinburgh.</p>
<p>So we could have one database master for each electoral district (or contiguous group of districts) with no real problems except that volunteers working at the very edge of an electoral district would only be supported to canvas on one side of the boundary. I’d rather find an architectural solution which works for the whole of Scotland, but if we cannot do that it isn’t a crisis.</p>
<p>It also should not be terribly difficult to organise for a street canvasser user using the <em>Map View</em> to be connected automatically to right geographic shard server, without any action by the user. The issue for telephone canvasser users is a bit different because they will often - perhaps typically - be canvassing voters in a region distant from where they are physically located, so if the geographic sharding model is adopted there would probably have to be an additional electoral district selection screen in the telephone canvasser’s interface.</p>
<p>Data from many ‘front-line’ database servers each serving a restricted geographic area can relatively simply be aggregated into a national server by doing the integration work in the wee sma’ oors, when most volunteers (and voters) are asleep.</p>
<p>The geographic sharding strategy is scalable. We could start with a single server, split it into a ‘west server’ and an ‘east server’ when that gets overloaded, and further subdivide as needed through the campaign. But we can only do this effectively if we have prepared and tested the strategy in advance.</p>
<p>But having considerable numbers of database servers will have cost implications.</p>
<h3><a href="#geographic-sharding-by-dns" name="geographic-sharding-by-dns"></a>Geographic sharding by DNS</h3>
<p>When I first thought of geographic sharding, I intended sharding by electoral district, but actually that makes no sense because electoral districts are complex polygons, which makes point-within-polygon computationally expensive. 4 degrees west falls just west of Stirling, and divides the country in half north-south. 56 degrees north runs north of Edinburgh and Glasgow, but just south of Falkirk. It divides the country in half east to west. Very few towns (and no large towns) straddle either line. Thus we can divide Scotland neatly into four, and it is computationally extremely cheap to compute which shard each data item should be despatched to.</p>
<p>We can then set up in DNS four addresses:</p>
<pre><code>+----------------------+-----------+-----------+
| Address              | longitude | latitude  |
+----------------------+-----------+-----------+
| nw.data.yyy.scot     | &lt; -4      | &gt;= 56     |
+----------------------+-----------+-----------+
| ne.data.yyy.scot     | &gt;= -4     | &gt;= 56     |
+----------------------+-----------+-----------+
| sw.data.yyy.scot     | &lt; -4      | &lt; 56      |
+----------------------+-----------+-----------+
| se.data.yyy.scot     | &gt;= -4     | &lt; 56      |
+----------------------+-----------+-----------+
</code></pre>
<p>giving us an incredibly simple dispatch table. Furthermore, initially all four addresses can point to the same server. This is an incredibly simple scheme, and I’m confident it’s good enough.</p>
<p>Data that’s inserted from the canvassing app - that is to say, voter intention data and followup request data - should have an additional field ‘shard’ (char(2)) which should hold the digraph representing the shard to which it was dispatched, and that field should form part of the primary key, allowing the data from all servers to be integrated. Data that isn’t from the canvassing app should probably be directed to the ‘nw’ shard (which will be lightest loaded), or to a separate master server, and then all servers should be synced overnight.</p>
<h3><a href="#read-servers-and-write-servers" name="read-servers-and-write-servers"></a>Read servers and write servers</h3>
<p>It’s a common practice in architecting busy web systems to have one master database server to which all write operations are directed, surrounded by a ring of slave databases which replicate from the master and serve all read requests. This works because for the majority of web systems there are many more reads than writes.</p>
<p>My feeling is that it’s likely that YouYesYet would see more writes than reads. Thus the ‘write to master, read from slaves’ pattern probably isn’t a big win. That isn’t to say that every database master should not have a ‘hot failover’ slave replicating from it which can take over immediately if the master goes down.</p>
<h3><a href="#app-servers-and-database-servers" name="app-servers-and-database-servers"></a>App servers and database servers</h3>
<p>The majority of the processing in YouYesYet happens client side; most of what is being sent back to the server is data to be stored directly in the database. So although there will be a small performance win in separating the app server from the database server this isn’t a very big win either.</p>
<h3><a href="#summary-spreading-the-load" name="summary-spreading-the-load"></a>Summary: spreading the load</h3>
<p>From the above I think the scaling problem should be addressed as follows:</p>
<ol>
  <li>Start with a pair of database servers (master and hot failover, with replication) and a single app server;</li>
  <li>Add additional app servers on a load-balancing basis as needed;</li>
  <li>Add a third database server (‘read server’), also replicating from the master, and direct reads to this;</li>
  <li>When the initial cluster of three database servers becomes overloaded, shard into two identical groups (‘east’ and ‘west’);</li>
  <li>When any shard becomes overloaded, split it into two further shards.</li>
</ol>
<p>If we have prepared for sharding, all that is required is to duplicate the database.</p>
<p>Obviously, once we have split the database into multiple shards, there is a task to integrate the data from the multiple shards in order to create an ‘across Scotland’ overview of the canvas data; however, again if we have prepared for it in advance, merging the databases should not be difficult, and can be done either in the wee sma’ oors or alternatively during the working day, as the system will be relatively lighty loaded during these periods.</p>
<h2><a href="#preparing-for-sharding" name="preparing-for-sharding"></a>Preparing for sharding</h2>
<p>We should prepare a Docker image for the app server and an image or setup script for the database server.</p>
<p>## Further reading on optimising Postgres performance</p>
<ol>
  <li><a href="https://wiki.postgresql.org/wiki/Replication,_Clustering,_and_Connection_Pooling">Replication, Clustering, and Connection Pooling</a></li>
</ol></div></div></div></body></html>